\chapter{Introduction}
In the past decade, the world has experienced one of the most exciting periods in computer development. Computer performance improvements have been dramatic
- a trend that promises to continue for the next several years. One reason for the improved performance is the rapid advancement in microprocessor technology.
Microprocessors have become smaller, denser and more powerful. The result is that microprocessor based super computing is rapidly becoming the technology of
preference in attacking some of the most important problems of science and engineering. To exploit microprocessor technology, vendors have developed high
parallel computers [2,9].\par
\hspace{1in}Highly parallel systems offer the enormous computational power needed for solving some of the most challenging computational problems such as
circuit simulation incorporating various effects. Unfortunately, software development has not keep pace with hardware advances. New programming paradigms
languages, scheduling and partitioning techniques, and algorithms are needed to fully exploit the power of these highly parallel machines.\par
\hspace{1in}A major new trend for scientific problem solving is distributed computing. In distributed computing [10], computers are connected by a network
are used collectively to solve a single larger problem. Many scientists are discovering that their computational requirements are best served not by a single,
monolithic computer but by a variety of distributed computing resources, linked by high speed networks. By parallel computing, we mean a set of processes that
are able to work together to solve a computational problem. There are a few things that are worthwhile to point out. First, the use of parallel processing and
the techniques that exploit them are now everywhere; from the personal computer to the fastest computer available. Second, parallel processing doesn't
necessary imply high performance computing.

\section{Traditional computers and their limitation}

The traditional computer, or conventional approach to computer design involves a single instruction stream. Instructions are processed sequentially and the
result is movement of data from memory to functional unit and back to memory. As demands for faster performance increased, modifications were made to
improve the design of the computers. It became evident that a number of factors were limiting potential speed: the switching speed of the devices, packaging
and interconnection delays, and compromises in the design to account for realistic tolerances of parameters in the timing of individual components. Even if a
dramatic improvement could be made in any of these areas, one factor still limits performance: the speed of light. Today's supercomputers have a cycle time
on the order of nanoseconds. One nanosecond translates into the time it takes light to move about a foot (in practice, the speed of pulses through the wiring
of a computer ranges from 0.3 to 0.9 feet per nanosecond). Faced by this fundamental limitation, computer designers have begun moving in the direction of
parallelism. \par
\hspace{1in} In this report we explore some of the issues involved in the use of high performance computing and parallel processing aspects. The organization
of the report is given below.

\section{Organization Of the Report}

\begin{enumerate}
\item Chapter 2 describes the fundamentals of parallel processing and issues related with high performance computing [1,2,9,10].
\item Chapter 3 describes the techniques used to decrease overhead and improve performance[10].
\item Chapter 4 describes the performance analysis for uniprocessor and parallel processors[2,10].
\item Chapter 5 describes parallel algorithms for solving linear equations[10].
\item Chapter 6 discusses future work that is to be done.
\end{enumerate}


